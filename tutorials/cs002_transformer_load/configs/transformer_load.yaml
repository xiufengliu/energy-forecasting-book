# Transformer Load Forecasting Configuration
# Reproduces Chapter 6 results

n_samples: 8760
lookback: 168
horizon: 24
seed: 42

# Transformer architecture
d_model: 64
nhead: 4
num_layers: 2
dropout: 0.1

# Training
batch_size: 32
learning_rate: 0.001
epochs: 100
patience: 10
grad_clip: 1.0

# Expected performance: WAPE 2.2% - 3.5%
